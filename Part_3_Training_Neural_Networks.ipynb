{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gRj-x7h332N"
   },
   "source": [
    "\n",
    "# Training Neural Networks\n",
    "\n",
    "The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-bEg-Zz4Q7z"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Note:** I'm glossing over a few details here that require some knowledge of vector calculus, but they aren't necessary to understand what's going on.\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "worDfYepJH6j"
   },
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdhxHwr57Yn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yCtUH8paXqBQ",
    "outputId": "1a4c93cf-21a8-4574-d121-f238912d28e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 2.4.0\n",
      "\t• tf.keras version: 2.4.0\n",
      "\t• GPU device not found. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zQV8MLaJOjN"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "Att74swb7Ol0",
    "outputId": "a98f6ee1-9881-4d8d-8766-b8b00a2cb4f8"
   },
   "outputs": [],
   "source": [
    "training_set, dataset_info = tfds.load('mnist', split='train', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiSe5BPrJquE"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r4EMOdT9pM3"
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "num_training_examples = dataset_info.splits['train'].num_examples\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9SC4gnUJucy"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo2DfMVvAdbd"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TCpaAlcKCDB"
   },
   "source": [
    "## Getting the Model Ready For Training\n",
    "\n",
    "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
    "\n",
    "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "These are the main parameters we are going to set throught these lesson. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYv3pv5-InR1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5CjYa8ES3OI"
   },
   "source": [
    "## Taking a Look at the Loss and Accuracy Before Training\n",
    "\n",
    "Before we train our model, let's take a look at how our model performs when it is just using random weights. Let's take a look at the `loss` and `accuracy` values when we pass a single batch of images to our un-trained model. To do this, we will use the `.evaluate(data, true_labels)` method. The `.evaluate(data, true_labels)` method compares the predicted output of our model on the given `data` with the given `true_labels` and returns the `loss` and `accuracy` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "u_7aijzvJQZ7",
    "outputId": "f66f355e-d030-4c30-e50c-7bba125a20cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.90 - 0s 10ms/step - loss: 0.1073 - accuracy: 0.9531\n",
      "\n",
      "Loss before training: 0.107\n",
      "Accuracy before training: 95.312%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvsfbLEMZjZ5"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Z-CgmnKBZDjq",
    "outputId": "38ab455c-767a-4705-c172-9d7cc926c239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - ETA: 23:17 - loss: 2.3312 - accuracy: 0.093 - ETA: 6s - loss: 2.0970 - accuracy: 0.3090  - ETA: 4s - loss: 1.6954 - accuracy: 0.52 - ETA: 4s - loss: 1.3872 - accuracy: 0.62 - ETA: 4s - loss: 1.2439 - accuracy: 0.66 - ETA: 4s - loss: 1.0869 - accuracy: 0.70 - ETA: 4s - loss: 1.0156 - accuracy: 0.72 - ETA: 4s - loss: 0.9178 - accuracy: 0.74 - ETA: 4s - loss: 0.8567 - accuracy: 0.76 - ETA: 4s - loss: 0.7975 - accuracy: 0.78 - ETA: 4s - loss: 0.7648 - accuracy: 0.78 - ETA: 4s - loss: 0.7254 - accuracy: 0.79 - ETA: 4s - loss: 0.6997 - accuracy: 0.80 - ETA: 4s - loss: 0.6886 - accuracy: 0.80 - ETA: 4s - loss: 0.6664 - accuracy: 0.81 - ETA: 4s - loss: 0.6437 - accuracy: 0.82 - ETA: 4s - loss: 0.6332 - accuracy: 0.82 - ETA: 4s - loss: 0.6166 - accuracy: 0.82 - ETA: 4s - loss: 0.6005 - accuracy: 0.83 - ETA: 4s - loss: 0.5803 - accuracy: 0.83 - ETA: 4s - loss: 0.5629 - accuracy: 0.84 - ETA: 3s - loss: 0.5452 - accuracy: 0.84 - ETA: 3s - loss: 0.5394 - accuracy: 0.84 - ETA: 3s - loss: 0.5260 - accuracy: 0.85 - ETA: 3s - loss: 0.5209 - accuracy: 0.85 - ETA: 3s - loss: 0.5113 - accuracy: 0.85 - ETA: 3s - loss: 0.4983 - accuracy: 0.86 - ETA: 3s - loss: 0.4868 - accuracy: 0.86 - ETA: 3s - loss: 0.4785 - accuracy: 0.86 - ETA: 3s - loss: 0.4731 - accuracy: 0.86 - ETA: 3s - loss: 0.4635 - accuracy: 0.86 - ETA: 3s - loss: 0.4560 - accuracy: 0.87 - ETA: 3s - loss: 0.4505 - accuracy: 0.87 - ETA: 3s - loss: 0.4427 - accuracy: 0.87 - ETA: 3s - loss: 0.4370 - accuracy: 0.87 - ETA: 3s - loss: 0.4302 - accuracy: 0.87 - ETA: 2s - loss: 0.4225 - accuracy: 0.88 - ETA: 2s - loss: 0.4175 - accuracy: 0.88 - ETA: 2s - loss: 0.4122 - accuracy: 0.88 - ETA: 2s - loss: 0.4089 - accuracy: 0.88 - ETA: 2s - loss: 0.4038 - accuracy: 0.88 - ETA: 2s - loss: 0.3975 - accuracy: 0.88 - ETA: 2s - loss: 0.3927 - accuracy: 0.88 - ETA: 2s - loss: 0.3887 - accuracy: 0.88 - ETA: 2s - loss: 0.3834 - accuracy: 0.89 - ETA: 2s - loss: 0.3801 - accuracy: 0.89 - ETA: 2s - loss: 0.3751 - accuracy: 0.89 - ETA: 2s - loss: 0.3701 - accuracy: 0.89 - ETA: 2s - loss: 0.3662 - accuracy: 0.89 - ETA: 2s - loss: 0.3621 - accuracy: 0.89 - ETA: 2s - loss: 0.3600 - accuracy: 0.89 - ETA: 2s - loss: 0.3573 - accuracy: 0.89 - ETA: 2s - loss: 0.3572 - accuracy: 0.89 - ETA: 2s - loss: 0.3530 - accuracy: 0.89 - ETA: 1s - loss: 0.3502 - accuracy: 0.89 - ETA: 1s - loss: 0.3476 - accuracy: 0.90 - ETA: 1s - loss: 0.3445 - accuracy: 0.90 - ETA: 1s - loss: 0.3412 - accuracy: 0.90 - ETA: 1s - loss: 0.3380 - accuracy: 0.90 - ETA: 1s - loss: 0.3343 - accuracy: 0.90 - ETA: 1s - loss: 0.3321 - accuracy: 0.90 - ETA: 1s - loss: 0.3291 - accuracy: 0.90 - ETA: 1s - loss: 0.3265 - accuracy: 0.90 - ETA: 1s - loss: 0.3239 - accuracy: 0.90 - ETA: 1s - loss: 0.3213 - accuracy: 0.90 - ETA: 1s - loss: 0.3180 - accuracy: 0.90 - ETA: 1s - loss: 0.3159 - accuracy: 0.90 - ETA: 1s - loss: 0.3142 - accuracy: 0.90 - ETA: 1s - loss: 0.3120 - accuracy: 0.91 - ETA: 1s - loss: 0.3095 - accuracy: 0.91 - ETA: 0s - loss: 0.3073 - accuracy: 0.91 - ETA: 0s - loss: 0.3056 - accuracy: 0.91 - ETA: 0s - loss: 0.3034 - accuracy: 0.91 - ETA: 0s - loss: 0.3015 - accuracy: 0.91 - ETA: 0s - loss: 0.2989 - accuracy: 0.91 - ETA: 0s - loss: 0.2966 - accuracy: 0.91 - ETA: 0s - loss: 0.2945 - accuracy: 0.91 - ETA: 0s - loss: 0.2920 - accuracy: 0.91 - ETA: 0s - loss: 0.2898 - accuracy: 0.91 - ETA: 0s - loss: 0.2877 - accuracy: 0.91 - ETA: 0s - loss: 0.2862 - accuracy: 0.91 - ETA: 0s - loss: 0.2843 - accuracy: 0.91 - ETA: 0s - loss: 0.2824 - accuracy: 0.91 - ETA: 0s - loss: 0.2808 - accuracy: 0.91 - ETA: 0s - loss: 0.2799 - accuracy: 0.91 - ETA: 0s - loss: 0.2781 - accuracy: 0.91 - 6s 5ms/step - loss: 0.2777 - accuracy: 0.9198\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - ETA: 33s - loss: 0.1651 - accuracy: 0.968 - ETA: 4s - loss: 0.1724 - accuracy: 0.946 - ETA: 3s - loss: 0.1361 - accuracy: 0.95 - ETA: 3s - loss: 0.1351 - accuracy: 0.95 - ETA: 3s - loss: 0.1346 - accuracy: 0.95 - ETA: 3s - loss: 0.1260 - accuracy: 0.96 - ETA: 3s - loss: 0.1295 - accuracy: 0.95 - ETA: 3s - loss: 0.1269 - accuracy: 0.96 - ETA: 3s - loss: 0.1293 - accuracy: 0.95 - ETA: 3s - loss: 0.1294 - accuracy: 0.96 - ETA: 3s - loss: 0.1323 - accuracy: 0.95 - ETA: 3s - loss: 0.1294 - accuracy: 0.96 - ETA: 3s - loss: 0.1303 - accuracy: 0.96 - ETA: 3s - loss: 0.1296 - accuracy: 0.96 - ETA: 3s - loss: 0.1302 - accuracy: 0.96 - ETA: 3s - loss: 0.1313 - accuracy: 0.96 - ETA: 3s - loss: 0.1305 - accuracy: 0.96 - ETA: 3s - loss: 0.1293 - accuracy: 0.96 - ETA: 3s - loss: 0.1286 - accuracy: 0.96 - ETA: 2s - loss: 0.1294 - accuracy: 0.96 - ETA: 2s - loss: 0.1289 - accuracy: 0.96 - ETA: 2s - loss: 0.1289 - accuracy: 0.96 - ETA: 2s - loss: 0.1282 - accuracy: 0.96 - ETA: 2s - loss: 0.1273 - accuracy: 0.96 - ETA: 2s - loss: 0.1271 - accuracy: 0.96 - ETA: 2s - loss: 0.1265 - accuracy: 0.96 - ETA: 2s - loss: 0.1272 - accuracy: 0.96 - ETA: 2s - loss: 0.1266 - accuracy: 0.96 - ETA: 2s - loss: 0.1278 - accuracy: 0.96 - ETA: 2s - loss: 0.1267 - accuracy: 0.96 - ETA: 2s - loss: 0.1264 - accuracy: 0.96 - ETA: 2s - loss: 0.1261 - accuracy: 0.96 - ETA: 2s - loss: 0.1255 - accuracy: 0.96 - ETA: 2s - loss: 0.1261 - accuracy: 0.96 - ETA: 2s - loss: 0.1262 - accuracy: 0.96 - ETA: 2s - loss: 0.1255 - accuracy: 0.96 - ETA: 2s - loss: 0.1247 - accuracy: 0.96 - ETA: 2s - loss: 0.1241 - accuracy: 0.96 - ETA: 1s - loss: 0.1240 - accuracy: 0.96 - ETA: 1s - loss: 0.1234 - accuracy: 0.96 - ETA: 1s - loss: 0.1233 - accuracy: 0.96 - ETA: 1s - loss: 0.1230 - accuracy: 0.96 - ETA: 1s - loss: 0.1222 - accuracy: 0.96 - ETA: 1s - loss: 0.1219 - accuracy: 0.96 - ETA: 1s - loss: 0.1215 - accuracy: 0.96 - ETA: 1s - loss: 0.1216 - accuracy: 0.96 - ETA: 1s - loss: 0.1214 - accuracy: 0.96 - ETA: 1s - loss: 0.1213 - accuracy: 0.96 - ETA: 1s - loss: 0.1207 - accuracy: 0.96 - ETA: 1s - loss: 0.1208 - accuracy: 0.96 - ETA: 1s - loss: 0.1206 - accuracy: 0.96 - ETA: 1s - loss: 0.1203 - accuracy: 0.96 - ETA: 1s - loss: 0.1196 - accuracy: 0.96 - ETA: 1s - loss: 0.1191 - accuracy: 0.96 - ETA: 1s - loss: 0.1185 - accuracy: 0.96 - ETA: 1s - loss: 0.1183 - accuracy: 0.96 - ETA: 1s - loss: 0.1183 - accuracy: 0.96 - ETA: 1s - loss: 0.1184 - accuracy: 0.96 - ETA: 1s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1175 - accuracy: 0.96 - ETA: 0s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1174 - accuracy: 0.96 - ETA: 0s - loss: 0.1172 - accuracy: 0.96 - ETA: 0s - loss: 0.1176 - accuracy: 0.96 - ETA: 0s - loss: 0.1167 - accuracy: 0.96 - ETA: 0s - loss: 0.1159 - accuracy: 0.96 - ETA: 0s - loss: 0.1157 - accuracy: 0.96 - ETA: 0s - loss: 0.1154 - accuracy: 0.96 - ETA: 0s - loss: 0.1152 - accuracy: 0.96 - ETA: 0s - loss: 0.1155 - accuracy: 0.96 - ETA: 0s - loss: 0.1151 - accuracy: 0.96 - ETA: 0s - loss: 0.1154 - accuracy: 0.96 - ETA: 0s - loss: 0.1156 - accuracy: 0.96 - ETA: 0s - loss: 0.1149 - accuracy: 0.96 - ETA: 0s - loss: 0.1147 - accuracy: 0.96 - 4s 4ms/step - loss: 0.1147 - accuracy: 0.9652\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - ETA: 27s - loss: 0.1283 - accuracy: 0.937 - ETA: 3s - loss: 0.0839 - accuracy: 0.971 - ETA: 3s - loss: 0.0842 - accuracy: 0.97 - ETA: 3s - loss: 0.0764 - accuracy: 0.97 - ETA: 3s - loss: 0.0720 - accuracy: 0.97 - ETA: 3s - loss: 0.0674 - accuracy: 0.97 - ETA: 3s - loss: 0.0701 - accuracy: 0.97 - ETA: 3s - loss: 0.0725 - accuracy: 0.97 - ETA: 3s - loss: 0.0744 - accuracy: 0.97 - ETA: 3s - loss: 0.0735 - accuracy: 0.97 - ETA: 3s - loss: 0.0760 - accuracy: 0.97 - ETA: 3s - loss: 0.0785 - accuracy: 0.97 - ETA: 3s - loss: 0.0786 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0830 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 3s - loss: 0.0845 - accuracy: 0.97 - ETA: 3s - loss: 0.0841 - accuracy: 0.97 - ETA: 3s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0826 - accuracy: 0.97 - ETA: 2s - loss: 0.0831 - accuracy: 0.97 - ETA: 2s - loss: 0.0830 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0818 - accuracy: 0.97 - ETA: 2s - loss: 0.0813 - accuracy: 0.97 - ETA: 2s - loss: 0.0809 - accuracy: 0.97 - ETA: 2s - loss: 0.0816 - accuracy: 0.97 - ETA: 2s - loss: 0.0818 - accuracy: 0.97 - ETA: 2s - loss: 0.0823 - accuracy: 0.97 - ETA: 2s - loss: 0.0826 - accuracy: 0.97 - ETA: 2s - loss: 0.0825 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0817 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0808 - accuracy: 0.97 - ETA: 2s - loss: 0.0810 - accuracy: 0.97 - ETA: 1s - loss: 0.0807 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0803 - accuracy: 0.97 - ETA: 1s - loss: 0.0808 - accuracy: 0.97 - ETA: 1s - loss: 0.0805 - accuracy: 0.97 - ETA: 1s - loss: 0.0807 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0800 - accuracy: 0.97 - ETA: 1s - loss: 0.0797 - accuracy: 0.97 - ETA: 1s - loss: 0.0797 - accuracy: 0.97 - ETA: 1s - loss: 0.0800 - accuracy: 0.97 - ETA: 1s - loss: 0.0803 - accuracy: 0.97 - ETA: 1s - loss: 0.0805 - accuracy: 0.97 - ETA: 1s - loss: 0.0805 - accuracy: 0.97 - ETA: 1s - loss: 0.0805 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0800 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0799 - accuracy: 0.97 - ETA: 1s - loss: 0.0798 - accuracy: 0.97 - ETA: 1s - loss: 0.0800 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0800 - accuracy: 0.97 - ETA: 0s - loss: 0.0797 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - 4s 4ms/step - loss: 0.0794 - accuracy: 0.9757\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - ETA: 34s - loss: 0.0132 - accuracy: 1.000 - ETA: 3s - loss: 0.0736 - accuracy: 0.975 - ETA: 3s - loss: 0.0725 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 3s - loss: 0.0639 - accuracy: 0.98 - ETA: 3s - loss: 0.0580 - accuracy: 0.98 - ETA: 3s - loss: 0.0585 - accuracy: 0.98 - ETA: 3s - loss: 0.0561 - accuracy: 0.98 - ETA: 3s - loss: 0.0568 - accuracy: 0.98 - ETA: 3s - loss: 0.0585 - accuracy: 0.98 - ETA: 3s - loss: 0.0586 - accuracy: 0.98 - ETA: 3s - loss: 0.0576 - accuracy: 0.98 - ETA: 3s - loss: 0.0596 - accuracy: 0.98 - ETA: 3s - loss: 0.0597 - accuracy: 0.98 - ETA: 3s - loss: 0.0592 - accuracy: 0.98 - ETA: 3s - loss: 0.0584 - accuracy: 0.98 - ETA: 3s - loss: 0.0578 - accuracy: 0.98 - ETA: 3s - loss: 0.0573 - accuracy: 0.98 - ETA: 2s - loss: 0.0580 - accuracy: 0.98 - ETA: 2s - loss: 0.0574 - accuracy: 0.98 - ETA: 2s - loss: 0.0587 - accuracy: 0.98 - ETA: 2s - loss: 0.0589 - accuracy: 0.98 - ETA: 2s - loss: 0.0588 - accuracy: 0.98 - ETA: 2s - loss: 0.0586 - accuracy: 0.98 - ETA: 2s - loss: 0.0585 - accuracy: 0.98 - ETA: 2s - loss: 0.0582 - accuracy: 0.98 - ETA: 2s - loss: 0.0584 - accuracy: 0.98 - ETA: 2s - loss: 0.0583 - accuracy: 0.98 - ETA: 2s - loss: 0.0593 - accuracy: 0.98 - ETA: 2s - loss: 0.0595 - accuracy: 0.98 - ETA: 2s - loss: 0.0593 - accuracy: 0.98 - ETA: 2s - loss: 0.0594 - accuracy: 0.98 - ETA: 2s - loss: 0.0597 - accuracy: 0.98 - ETA: 2s - loss: 0.0593 - accuracy: 0.98 - ETA: 2s - loss: 0.0592 - accuracy: 0.98 - ETA: 2s - loss: 0.0592 - accuracy: 0.98 - ETA: 2s - loss: 0.0591 - accuracy: 0.98 - ETA: 2s - loss: 0.0588 - accuracy: 0.98 - ETA: 2s - loss: 0.0589 - accuracy: 0.98 - ETA: 1s - loss: 0.0587 - accuracy: 0.98 - ETA: 1s - loss: 0.0583 - accuracy: 0.98 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0579 - accuracy: 0.98 - ETA: 1s - loss: 0.0581 - accuracy: 0.98 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0581 - accuracy: 0.98 - ETA: 1s - loss: 0.0583 - accuracy: 0.98 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0581 - accuracy: 0.98 - ETA: 1s - loss: 0.0581 - accuracy: 0.98 - ETA: 1s - loss: 0.0584 - accuracy: 0.98 - ETA: 1s - loss: 0.0585 - accuracy: 0.98 - ETA: 1s - loss: 0.0590 - accuracy: 0.98 - ETA: 1s - loss: 0.0589 - accuracy: 0.98 - ETA: 1s - loss: 0.0593 - accuracy: 0.98 - ETA: 1s - loss: 0.0590 - accuracy: 0.98 - ETA: 1s - loss: 0.0590 - accuracy: 0.98 - ETA: 1s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0587 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0591 - accuracy: 0.98 - ETA: 0s - loss: 0.0587 - accuracy: 0.98 - ETA: 0s - loss: 0.0586 - accuracy: 0.98 - ETA: 0s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0591 - accuracy: 0.98 - ETA: 0s - loss: 0.0592 - accuracy: 0.98 - ETA: 0s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0588 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.98 - ETA: 0s - loss: 0.0587 - accuracy: 0.98 - ETA: 0s - loss: 0.0587 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.98 - ETA: 0s - loss: 0.0586 - accuracy: 0.98 - ETA: 0s - loss: 0.0584 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0586 - accuracy: 0.9821\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - ETA: 24s - loss: 0.0189 - accuracy: 1.000 - ETA: 4s - loss: 0.0446 - accuracy: 0.984 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0485 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 4s - loss: 0.0527 - accuracy: 0.98 - ETA: 4s - loss: 0.0507 - accuracy: 0.98 - ETA: 4s - loss: 0.0499 - accuracy: 0.98 - ETA: 4s - loss: 0.0500 - accuracy: 0.98 - ETA: 4s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0517 - accuracy: 0.98 - ETA: 3s - loss: 0.0521 - accuracy: 0.98 - ETA: 3s - loss: 0.0502 - accuracy: 0.98 - ETA: 3s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0520 - accuracy: 0.98 - ETA: 3s - loss: 0.0513 - accuracy: 0.98 - ETA: 3s - loss: 0.0513 - accuracy: 0.98 - ETA: 3s - loss: 0.0504 - accuracy: 0.98 - ETA: 3s - loss: 0.0509 - accuracy: 0.98 - ETA: 3s - loss: 0.0521 - accuracy: 0.98 - ETA: 3s - loss: 0.0511 - accuracy: 0.98 - ETA: 3s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0515 - accuracy: 0.98 - ETA: 3s - loss: 0.0511 - accuracy: 0.98 - ETA: 3s - loss: 0.0507 - accuracy: 0.98 - ETA: 3s - loss: 0.0504 - accuracy: 0.98 - ETA: 3s - loss: 0.0499 - accuracy: 0.98 - ETA: 2s - loss: 0.0504 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0503 - accuracy: 0.98 - ETA: 2s - loss: 0.0498 - accuracy: 0.98 - ETA: 2s - loss: 0.0489 - accuracy: 0.98 - ETA: 2s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0473 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0474 - accuracy: 0.98 - ETA: 2s - loss: 0.0474 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0470 - accuracy: 0.98 - ETA: 2s - loss: 0.0471 - accuracy: 0.98 - ETA: 2s - loss: 0.0473 - accuracy: 0.98 - ETA: 2s - loss: 0.0475 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0480 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0476 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0470 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0465 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - 4s 5ms/step - loss: 0.0469 - accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFgG_WfUjCic"
   },
   "source": [
    "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson. \n",
    "\n",
    "With our model trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "ghr7z-SnctRw",
    "outputId": "8e946c9a-56b5-45f4-e79f-c6451ff8b7d5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "  \n",
    "  \n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4WcPdR9jKMB"
   },
   "source": [
    "WOW!! Now our network is brilliant. It can accurately predict the digits in our images. Let's take a look again at the loss and accuracy values for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "rFZKBfTgfPVy",
    "outputId": "b4d7816a-bbfa-4bb8-c453-82506029aeb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 1.00 - 0s 8ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "\n",
      "Loss after training: 0.012\n",
      "Accuracy after training: 100.000%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
    "print('Accuracy after training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wa5_vwtotNeg"
   },
   "source": [
    "> **Exercise:** Create a network with 784 input units, a hidden layer with 128 units, then a hidden layer with 64 units, then a hidden layer with 32 units and finally an output layer with 10 units. Use a ReLu activation function for all the hidden layers and a softmax activation function for the output layer. Then compile the model using an `adam` optimizer, a `sparse_categorical_crossentropy` loss function, and the `accuracy` metric. Finally, print the loss and accuracy of your un-trained model for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "txuSaeuirvgc",
    "outputId": "33af7e60-e292-4788-f168-940351b7e6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 2.3736 - accuracy: 0.12 - 0s 6ms/step - loss: 2.3440 - accuracy: 0.1458\n",
      "\n",
      "Loss before training: 2.329\n",
      "Accuracy before training: 15.625%\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "model_1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model_1.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgdaQEVUumxo"
   },
   "source": [
    "> **Exercise:** Train the model you created above for 5 epochs and then print the loss and accuracy of your trained model for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "HzxZtgBDt3Ak",
    "outputId": "e487178a-e5dd-411b-e5dc-566983d4aa27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - ETA: 9:37 - loss: 2.3208 - accuracy: 0.12 - ETA: 3s - loss: 2.0910 - accuracy: 0.3498 - ETA: 3s - loss: 1.8198 - accuracy: 0.44 - ETA: 3s - loss: 1.5739 - accuracy: 0.52 - ETA: 3s - loss: 1.3566 - accuracy: 0.59 - ETA: 3s - loss: 1.1711 - accuracy: 0.65 - ETA: 3s - loss: 1.0568 - accuracy: 0.68 - ETA: 3s - loss: 0.9665 - accuracy: 0.71 - ETA: 3s - loss: 0.9154 - accuracy: 0.72 - ETA: 3s - loss: 0.8609 - accuracy: 0.74 - ETA: 3s - loss: 0.8151 - accuracy: 0.75 - ETA: 3s - loss: 0.7791 - accuracy: 0.76 - ETA: 3s - loss: 0.7450 - accuracy: 0.77 - ETA: 3s - loss: 0.7182 - accuracy: 0.78 - ETA: 3s - loss: 0.6871 - accuracy: 0.79 - ETA: 3s - loss: 0.6640 - accuracy: 0.80 - ETA: 3s - loss: 0.6371 - accuracy: 0.81 - ETA: 3s - loss: 0.6156 - accuracy: 0.82 - ETA: 3s - loss: 0.5973 - accuracy: 0.82 - ETA: 3s - loss: 0.5798 - accuracy: 0.82 - ETA: 3s - loss: 0.5607 - accuracy: 0.83 - ETA: 3s - loss: 0.5475 - accuracy: 0.83 - ETA: 3s - loss: 0.5329 - accuracy: 0.84 - ETA: 3s - loss: 0.5228 - accuracy: 0.84 - ETA: 2s - loss: 0.5104 - accuracy: 0.85 - ETA: 2s - loss: 0.4995 - accuracy: 0.85 - ETA: 2s - loss: 0.4892 - accuracy: 0.85 - ETA: 2s - loss: 0.4795 - accuracy: 0.85 - ETA: 2s - loss: 0.4702 - accuracy: 0.86 - ETA: 2s - loss: 0.4613 - accuracy: 0.86 - ETA: 2s - loss: 0.4535 - accuracy: 0.86 - ETA: 2s - loss: 0.4450 - accuracy: 0.86 - ETA: 2s - loss: 0.4384 - accuracy: 0.87 - ETA: 2s - loss: 0.4322 - accuracy: 0.87 - ETA: 2s - loss: 0.4259 - accuracy: 0.87 - ETA: 2s - loss: 0.4206 - accuracy: 0.87 - ETA: 2s - loss: 0.4136 - accuracy: 0.87 - ETA: 2s - loss: 0.4079 - accuracy: 0.87 - ETA: 2s - loss: 0.4030 - accuracy: 0.88 - ETA: 1s - loss: 0.3984 - accuracy: 0.88 - ETA: 1s - loss: 0.3936 - accuracy: 0.88 - ETA: 1s - loss: 0.3888 - accuracy: 0.88 - ETA: 1s - loss: 0.3843 - accuracy: 0.88 - ETA: 1s - loss: 0.3801 - accuracy: 0.88 - ETA: 1s - loss: 0.3762 - accuracy: 0.88 - ETA: 1s - loss: 0.3711 - accuracy: 0.89 - ETA: 1s - loss: 0.3665 - accuracy: 0.89 - ETA: 1s - loss: 0.3624 - accuracy: 0.89 - ETA: 1s - loss: 0.3591 - accuracy: 0.89 - ETA: 1s - loss: 0.3553 - accuracy: 0.89 - ETA: 1s - loss: 0.3512 - accuracy: 0.89 - ETA: 1s - loss: 0.3487 - accuracy: 0.89 - ETA: 1s - loss: 0.3459 - accuracy: 0.89 - ETA: 1s - loss: 0.3422 - accuracy: 0.89 - ETA: 1s - loss: 0.3399 - accuracy: 0.90 - ETA: 1s - loss: 0.3359 - accuracy: 0.90 - ETA: 1s - loss: 0.3327 - accuracy: 0.90 - ETA: 0s - loss: 0.3295 - accuracy: 0.90 - ETA: 0s - loss: 0.3264 - accuracy: 0.90 - ETA: 0s - loss: 0.3241 - accuracy: 0.90 - ETA: 0s - loss: 0.3213 - accuracy: 0.90 - ETA: 0s - loss: 0.3193 - accuracy: 0.90 - ETA: 0s - loss: 0.3162 - accuracy: 0.90 - ETA: 0s - loss: 0.3133 - accuracy: 0.90 - ETA: 0s - loss: 0.3115 - accuracy: 0.90 - ETA: 0s - loss: 0.3091 - accuracy: 0.90 - ETA: 0s - loss: 0.3061 - accuracy: 0.90 - ETA: 0s - loss: 0.3044 - accuracy: 0.91 - ETA: 0s - loss: 0.3023 - accuracy: 0.91 - ETA: 0s - loss: 0.2999 - accuracy: 0.91 - ETA: 0s - loss: 0.2984 - accuracy: 0.91 - ETA: 0s - loss: 0.2969 - accuracy: 0.91 - ETA: 0s - loss: 0.2947 - accuracy: 0.91 - ETA: 0s - loss: 0.2920 - accuracy: 0.91 - ETA: 0s - loss: 0.2899 - accuracy: 0.91 - ETA: 0s - loss: 0.2886 - accuracy: 0.91 - 5s 4ms/step - loss: 0.2881 - accuracy: 0.9149\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - ETA: 24s - loss: 0.1325 - accuracy: 0.953 - ETA: 3s - loss: 0.1815 - accuracy: 0.947 - ETA: 3s - loss: 0.1445 - accuracy: 0.95 - ETA: 3s - loss: 0.1419 - accuracy: 0.95 - ETA: 3s - loss: 0.1415 - accuracy: 0.96 - ETA: 3s - loss: 0.1411 - accuracy: 0.95 - ETA: 3s - loss: 0.1363 - accuracy: 0.96 - ETA: 3s - loss: 0.1383 - accuracy: 0.96 - ETA: 3s - loss: 0.1380 - accuracy: 0.96 - ETA: 3s - loss: 0.1347 - accuracy: 0.96 - ETA: 3s - loss: 0.1342 - accuracy: 0.96 - ETA: 3s - loss: 0.1325 - accuracy: 0.96 - ETA: 3s - loss: 0.1333 - accuracy: 0.96 - ETA: 3s - loss: 0.1367 - accuracy: 0.96 - ETA: 3s - loss: 0.1350 - accuracy: 0.96 - ETA: 3s - loss: 0.1331 - accuracy: 0.96 - ETA: 2s - loss: 0.1307 - accuracy: 0.96 - ETA: 2s - loss: 0.1288 - accuracy: 0.96 - ETA: 2s - loss: 0.1266 - accuracy: 0.96 - ETA: 2s - loss: 0.1264 - accuracy: 0.96 - ETA: 2s - loss: 0.1281 - accuracy: 0.96 - ETA: 2s - loss: 0.1272 - accuracy: 0.96 - ETA: 2s - loss: 0.1252 - accuracy: 0.96 - ETA: 2s - loss: 0.1245 - accuracy: 0.96 - ETA: 2s - loss: 0.1254 - accuracy: 0.96 - ETA: 2s - loss: 0.1259 - accuracy: 0.96 - ETA: 2s - loss: 0.1253 - accuracy: 0.96 - ETA: 2s - loss: 0.1246 - accuracy: 0.96 - ETA: 2s - loss: 0.1246 - accuracy: 0.96 - ETA: 2s - loss: 0.1246 - accuracy: 0.96 - ETA: 2s - loss: 0.1245 - accuracy: 0.96 - ETA: 2s - loss: 0.1244 - accuracy: 0.96 - ETA: 2s - loss: 0.1244 - accuracy: 0.96 - ETA: 2s - loss: 0.1236 - accuracy: 0.96 - ETA: 2s - loss: 0.1245 - accuracy: 0.96 - ETA: 2s - loss: 0.1247 - accuracy: 0.96 - ETA: 2s - loss: 0.1239 - accuracy: 0.96 - ETA: 1s - loss: 0.1233 - accuracy: 0.96 - ETA: 1s - loss: 0.1223 - accuracy: 0.96 - ETA: 1s - loss: 0.1214 - accuracy: 0.96 - ETA: 1s - loss: 0.1209 - accuracy: 0.96 - ETA: 1s - loss: 0.1206 - accuracy: 0.96 - ETA: 1s - loss: 0.1209 - accuracy: 0.96 - ETA: 1s - loss: 0.1204 - accuracy: 0.96 - ETA: 1s - loss: 0.1203 - accuracy: 0.96 - ETA: 1s - loss: 0.1205 - accuracy: 0.96 - ETA: 1s - loss: 0.1203 - accuracy: 0.96 - ETA: 1s - loss: 0.1204 - accuracy: 0.96 - ETA: 1s - loss: 0.1203 - accuracy: 0.96 - ETA: 1s - loss: 0.1196 - accuracy: 0.96 - ETA: 1s - loss: 0.1192 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1190 - accuracy: 0.96 - ETA: 1s - loss: 0.1185 - accuracy: 0.96 - ETA: 1s - loss: 0.1186 - accuracy: 0.96 - ETA: 1s - loss: 0.1192 - accuracy: 0.96 - ETA: 1s - loss: 0.1196 - accuracy: 0.96 - ETA: 0s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1178 - accuracy: 0.96 - ETA: 0s - loss: 0.1174 - accuracy: 0.96 - ETA: 0s - loss: 0.1171 - accuracy: 0.96 - ETA: 0s - loss: 0.1171 - accuracy: 0.96 - ETA: 0s - loss: 0.1172 - accuracy: 0.96 - ETA: 0s - loss: 0.1167 - accuracy: 0.96 - ETA: 0s - loss: 0.1169 - accuracy: 0.96 - ETA: 0s - loss: 0.1164 - accuracy: 0.96 - ETA: 0s - loss: 0.1160 - accuracy: 0.96 - ETA: 0s - loss: 0.1156 - accuracy: 0.96 - ETA: 0s - loss: 0.1153 - accuracy: 0.96 - ETA: 0s - loss: 0.1156 - accuracy: 0.96 - ETA: 0s - loss: 0.1154 - accuracy: 0.96 - ETA: 0s - loss: 0.1152 - accuracy: 0.96 - 4s 4ms/step - loss: 0.1154 - accuracy: 0.9656\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - ETA: 30s - loss: 0.1928 - accuracy: 0.921 - ETA: 3s - loss: 0.0910 - accuracy: 0.966 - ETA: 3s - loss: 0.0904 - accuracy: 0.97 - ETA: 3s - loss: 0.0814 - accuracy: 0.97 - ETA: 3s - loss: 0.0844 - accuracy: 0.97 - ETA: 3s - loss: 0.0901 - accuracy: 0.97 - ETA: 3s - loss: 0.0912 - accuracy: 0.97 - ETA: 3s - loss: 0.0908 - accuracy: 0.97 - ETA: 3s - loss: 0.0883 - accuracy: 0.97 - ETA: 3s - loss: 0.0871 - accuracy: 0.97 - ETA: 3s - loss: 0.0870 - accuracy: 0.97 - ETA: 3s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0868 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0843 - accuracy: 0.97 - ETA: 3s - loss: 0.0854 - accuracy: 0.97 - ETA: 3s - loss: 0.0858 - accuracy: 0.97 - ETA: 3s - loss: 0.0862 - accuracy: 0.97 - ETA: 3s - loss: 0.0858 - accuracy: 0.97 - ETA: 3s - loss: 0.0845 - accuracy: 0.97 - ETA: 3s - loss: 0.0862 - accuracy: 0.97 - ETA: 3s - loss: 0.0857 - accuracy: 0.97 - ETA: 3s - loss: 0.0872 - accuracy: 0.97 - ETA: 3s - loss: 0.0866 - accuracy: 0.97 - ETA: 3s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0859 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 3s - loss: 0.0862 - accuracy: 0.97 - ETA: 3s - loss: 0.0871 - accuracy: 0.97 - ETA: 3s - loss: 0.0865 - accuracy: 0.97 - ETA: 2s - loss: 0.0859 - accuracy: 0.97 - ETA: 2s - loss: 0.0854 - accuracy: 0.97 - ETA: 2s - loss: 0.0851 - accuracy: 0.97 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0839 - accuracy: 0.97 - ETA: 2s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0839 - accuracy: 0.97 - ETA: 2s - loss: 0.0841 - accuracy: 0.97 - ETA: 2s - loss: 0.0833 - accuracy: 0.97 - ETA: 2s - loss: 0.0832 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0819 - accuracy: 0.97 - ETA: 2s - loss: 0.0812 - accuracy: 0.97 - ETA: 1s - loss: 0.0819 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 1s - loss: 0.0819 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0820 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 1s - loss: 0.0818 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0824 - accuracy: 0.97 - ETA: 1s - loss: 0.0829 - accuracy: 0.97 - ETA: 1s - loss: 0.0829 - accuracy: 0.97 - ETA: 1s - loss: 0.0826 - accuracy: 0.97 - ETA: 1s - loss: 0.0826 - accuracy: 0.97 - ETA: 1s - loss: 0.0827 - accuracy: 0.97 - ETA: 1s - loss: 0.0826 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0823 - accuracy: 0.97 - ETA: 0s - loss: 0.0822 - accuracy: 0.97 - ETA: 0s - loss: 0.0829 - accuracy: 0.97 - ETA: 0s - loss: 0.0827 - accuracy: 0.97 - ETA: 0s - loss: 0.0834 - accuracy: 0.97 - ETA: 0s - loss: 0.0833 - accuracy: 0.97 - ETA: 0s - loss: 0.0835 - accuracy: 0.97 - ETA: 0s - loss: 0.0832 - accuracy: 0.97 - ETA: 0s - loss: 0.0828 - accuracy: 0.97 - ETA: 0s - loss: 0.0826 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0822 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0816 - accuracy: 0.97 - ETA: 0s - loss: 0.0813 - accuracy: 0.97 - 4s 4ms/step - loss: 0.0814 - accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - ETA: 23s - loss: 0.1737 - accuracy: 0.937 - ETA: 3s - loss: 0.0479 - accuracy: 0.985 - ETA: 3s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0578 - accuracy: 0.98 - ETA: 3s - loss: 0.0582 - accuracy: 0.98 - ETA: 3s - loss: 0.0606 - accuracy: 0.98 - ETA: 3s - loss: 0.0612 - accuracy: 0.97 - ETA: 3s - loss: 0.0596 - accuracy: 0.98 - ETA: 3s - loss: 0.0604 - accuracy: 0.98 - ETA: 3s - loss: 0.0632 - accuracy: 0.97 - ETA: 3s - loss: 0.0627 - accuracy: 0.98 - ETA: 3s - loss: 0.0619 - accuracy: 0.98 - ETA: 3s - loss: 0.0620 - accuracy: 0.98 - ETA: 3s - loss: 0.0622 - accuracy: 0.98 - ETA: 3s - loss: 0.0622 - accuracy: 0.98 - ETA: 2s - loss: 0.0623 - accuracy: 0.98 - ETA: 2s - loss: 0.0608 - accuracy: 0.98 - ETA: 2s - loss: 0.0628 - accuracy: 0.98 - ETA: 2s - loss: 0.0622 - accuracy: 0.98 - ETA: 2s - loss: 0.0618 - accuracy: 0.98 - ETA: 2s - loss: 0.0621 - accuracy: 0.98 - ETA: 2s - loss: 0.0623 - accuracy: 0.98 - ETA: 2s - loss: 0.0621 - accuracy: 0.98 - ETA: 2s - loss: 0.0623 - accuracy: 0.98 - ETA: 2s - loss: 0.0622 - accuracy: 0.98 - ETA: 2s - loss: 0.0628 - accuracy: 0.98 - ETA: 2s - loss: 0.0626 - accuracy: 0.98 - ETA: 2s - loss: 0.0631 - accuracy: 0.98 - ETA: 2s - loss: 0.0628 - accuracy: 0.98 - ETA: 2s - loss: 0.0631 - accuracy: 0.98 - ETA: 2s - loss: 0.0629 - accuracy: 0.98 - ETA: 2s - loss: 0.0626 - accuracy: 0.98 - ETA: 2s - loss: 0.0617 - accuracy: 0.98 - ETA: 2s - loss: 0.0611 - accuracy: 0.98 - ETA: 2s - loss: 0.0614 - accuracy: 0.98 - ETA: 1s - loss: 0.0611 - accuracy: 0.98 - ETA: 1s - loss: 0.0611 - accuracy: 0.98 - ETA: 1s - loss: 0.0607 - accuracy: 0.98 - ETA: 1s - loss: 0.0606 - accuracy: 0.98 - ETA: 1s - loss: 0.0608 - accuracy: 0.98 - ETA: 1s - loss: 0.0603 - accuracy: 0.98 - ETA: 1s - loss: 0.0600 - accuracy: 0.98 - ETA: 1s - loss: 0.0604 - accuracy: 0.98 - ETA: 1s - loss: 0.0601 - accuracy: 0.98 - ETA: 1s - loss: 0.0603 - accuracy: 0.98 - ETA: 1s - loss: 0.0599 - accuracy: 0.98 - ETA: 1s - loss: 0.0595 - accuracy: 0.98 - ETA: 1s - loss: 0.0594 - accuracy: 0.98 - ETA: 1s - loss: 0.0590 - accuracy: 0.98 - ETA: 1s - loss: 0.0589 - accuracy: 0.98 - ETA: 1s - loss: 0.0591 - accuracy: 0.98 - ETA: 1s - loss: 0.0591 - accuracy: 0.98 - ETA: 1s - loss: 0.0591 - accuracy: 0.98 - ETA: 1s - loss: 0.0592 - accuracy: 0.98 - ETA: 1s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0589 - accuracy: 0.98 - ETA: 0s - loss: 0.0591 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0595 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0591 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0599 - accuracy: 0.98 - ETA: 0s - loss: 0.0604 - accuracy: 0.98 - ETA: 0s - loss: 0.0606 - accuracy: 0.98 - ETA: 0s - loss: 0.0608 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0611 - accuracy: 0.9809\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - ETA: 28s - loss: 0.0105 - accuracy: 1.000 - ETA: 3s - loss: 0.0239 - accuracy: 0.995 - ETA: 3s - loss: 0.0392 - accuracy: 0.99 - ETA: 3s - loss: 0.0428 - accuracy: 0.98 - ETA: 3s - loss: 0.0428 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0411 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0409 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0447 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0442 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0440 - accuracy: 0.98 - ETA: 2s - loss: 0.0439 - accuracy: 0.98 - ETA: 2s - loss: 0.0441 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0449 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0454 - accuracy: 0.98 - ETA: 2s - loss: 0.0455 - accuracy: 0.98 - ETA: 2s - loss: 0.0456 - accuracy: 0.98 - ETA: 2s - loss: 0.0458 - accuracy: 0.98 - ETA: 2s - loss: 0.0461 - accuracy: 0.98 - ETA: 2s - loss: 0.0458 - accuracy: 0.98 - ETA: 2s - loss: 0.0461 - accuracy: 0.98 - ETA: 2s - loss: 0.0466 - accuracy: 0.98 - ETA: 2s - loss: 0.0466 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 2s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0467 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0477 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0471 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0470 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0470 - accuracy: 0.98 - ETA: 0s - loss: 0.0471 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0476 - accuracy: 0.9854\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 1.00 - 0s 7ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "\n",
      "Loss after training: 0.015\n",
      "Accuracy after training: 100.000%\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "EPOCHS = 5\n",
    "history = model_1.fit(training_batches, epochs = EPOCHS)\n",
    "\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model_1.evaluate(image_batch, label_batch)\n",
    "\n",
    "\n",
    "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
    "print('Accuracy after training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfBqrMikvVCY"
   },
   "source": [
    "> **Exercise:** Plot the prediction of the model you created and trained above on a single image from the training set. Also plot the probability predicted by your model for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "NOwMUqYzvKtK",
    "outputId": "5f653945-0fe4-4699-e2cc-98e67e050dbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Solution\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model_1.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "   \n",
    "  \n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqREWxKKVwql"
   },
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Let's now take a minute to see how TensorFlow calculates and keeps track of the gradients needed for backpropagation. TensorFlow provides a class that records automatic differentiation operations, called `tf.GradientTape`. Automatic differentiation, also known as algorithmic differentiation or simply “autodiff”, is a family of techniques used by computers for efficiently and accurately evaluating derivatives of numeric functions.\n",
    "\n",
    "`tf.GradientTape` works by keeping track of operations performed on tensors that are being \"watched\". By default `tf.GradientTape` will automatically \"watch\" any trainable variables, such as the weights in our model. Trainable variables are those that have `trainable=True`. When we create a model with `tf.keras`, all of the parameters are initialized with `trainable = True`. Any tensor can also be manually \"watched\" by invoking the watch method.\n",
    "\n",
    "\n",
    "Let's see a simple example. Let's take the following equation:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "The derivative of `y` with respect to `x` is given by:\n",
    "\n",
    "$$\n",
    "\\frac{d y}{d x} = 2x\n",
    "$$\n",
    "\n",
    "Now, let's use `tf.GradientTape` to calculate the derivative of a tensor `y` with respect to a tensor `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2-ktpx5dVU3O",
    "outputId": "d4a54fba-61eb-4419-e9d9-8162785ef09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient calculated by tf.GradientTape:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "True Gradient:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "Maximum Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# Print the gradient calculated by tf.GradientTape\n",
    "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
    "\n",
    "# Print the actual gradient of y = x^2\n",
    "print('\\nTrue Gradient:\\n', true_grad)\n",
    "\n",
    "# Print the maximum difference between true and calculated gradient\n",
    "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgLCJaooV5Un"
   },
   "source": [
    "The `tf.GradientTape` class keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor.\n",
    "\n",
    "To know more about `tf.GradientTape` and trainable variables check the following links\n",
    "\n",
    "* [Gradient Tape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape)\n",
    "\n",
    "* [TensorFlow Variables](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable)\n",
    "\n",
    "Next up you'll write the code for training a neural network on a more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am0SvU9KWAD3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3 - Training Neural Networks (Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
